# Runs as ansible-playbook -vvvv cbjpt-stg.yml --vault-password-file ~/.vault_pass.txt
---

- name: Creation of "{{ prefix }}-{{ env }}" Couchbase cluster
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    region: "{{ region }}"
    ami: "{{ cb_45_ami }}"
# Adjust the counts, and start/stop indexes to adjust cluster size
# node 1 is stand alone for cluster creation so startindex1=3 and count1 is 1 less than 
# count2
    count1: 1
    stride1: 2
    startindex1: 3
    endindex1: 3
    count2: 2
    stride2: 2
    startindex2: 2
    endindex2: 4
  vars_files:
    - ../group_vars/stg
    - ../group_vars/vaults/stg
    - ../host_vars/cbjpt-stg
  tasks:
    # First instance has to be it's own group for couchbase cluster config further in the playbook
    - name: Launch instance for master node
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_1 }}"
        zone: "{{ az1 }}"
        count: 1
        volumes:
          - device_name: /dev/xvdb
            volume_size: 40
            ephemeral: ephemeral0
            delete_on_termination: true
          - device_name: /dev/xvdc
            volume_size: 40
            ephemeral: ephemeral1
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} 01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          service: "{{ service_tag }}"
          couchbase-profile: index
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                      - [ ephemeral1, null ]
                    runcmd:
                      - mdadm --create /dev/md0 --name=0 --level=0 --raid-devices=2 /dev/xvdb /dev/xvdc -R
                      - mdadm --detail --brief /dev/md0 > /etc/mdadm.conf
                      - parted /dev/md0 mklabel gpt
                      - parted /dev/md0 mkpart primary xfs 0% 100%
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: ec2

    - name: register dns
      route53:
        command: create
        overwrite: true
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}01.{{ env }}.ddc.io"
        type: CNAME
        ttl: 60
        value: "{{ item.private_dns_name }}"
      with_items: "{{ ec2.instances }}"

    - name: Add EP2 instances to host group
      add_host:
        hostname: "{{ item.private_ip }}"
        groupname: couchbase-main
      with_items: "{{ ec2.instances }}"

      # These are the remaining odd numbered instances that will not be the couchbase-main in cluster
    - name: Launch instances group 1
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_1 }}"
        zone: "{{ az1 }}"
        count: "{{ count1 }}"
        volumes:
          - device_name: /dev/xvdb
            volume_size: 40
            ephemeral: ephemeral0
            delete_on_termination: true
          - device_name: /dev/xvdc
            volume_size: 40
            ephemeral: ephemeral1
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} 01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          couchbase-profile: data
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                      - [ ephemeral1, null ]
                    runcmd:
                      - mdadm --create /dev/md0 --name=0 --level=0 --raid-devices=2 /dev/xvdb /dev/xvdc -R
                      - mdadm --detail --brief /dev/md0 > /etc/mdadm.conf
                      - parted /dev/md0 mklabel gpt
                      - parted /dev/md0 mkpart primary xfs 0% 100%
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: created1

    - name: remove any existing file before running just in case
      file:
        path: ./hosts.cb1
        state: absent

    - name: write instance ids and private ips of the instances to local hosts file
      lineinfile:
        dest: ./hosts.cb1
        line: "{{ item.id }} {{ item.private_ip }}"
        create: yes
      with_items: "{{ created1.instances }}"

      # This is done in the old format because of a ansible bug that still exists in version 2.4
    - name: create identifier sequence for tagging
      debug:
        msg: "{{ item }}"
      with_sequence: start={{ startindex1 }} end={{ endindex1 }} stride={{ stride1 }} format=%02d
      register: sequence1

    - name: tag instances
      ec2_tag:
        resource: "{{ item.0.id }}"
        region: "{{ region }}"
        tags:
          Name: "{{ prefix }}-{{ env }} {{ item.1.msg }}"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
      with_together:
        - "{{ created1.instances }}"
        - "{{ sequence1.results }}"

    - name: create dns records
      route53:
        command: create
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}{{ item.1.msg }}.{{ dd_env }}.{{ fqdn_suffix }}"
        type: CNAME
        ttl: 60
        value: "{{ item.0.private_dns_name }}"
        overwrite: true
      with_together:
        - "{{ created1.instances }}"
        - "{{ sequence1.results }}"

    - name: clean up after ourselves, remove the temp hosts file
      file:
        path: ./hosts.cb1
        state: absent

      # These are the even numbered instances 
    - name: Launch instances group 2
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_2 }}"
        zone: "{{ az2 }}"
        count: "{{ count2 }}"
        volumes:
          - device_name: /dev/xvdb
            volume_size: 40
            ephemeral: ephemeral0
            delete_on_termination: true
          - device_name: /dev/xvdc
            volume_size: 40
            ephemeral: ephemeral1
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} 01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          couchbase-profile: data
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                      - [ ephemeral1, null ]
                    runcmd:
                      - mdadm --create /dev/md0 --name=0 --level=0 --raid-devices=2 /dev/xvdb /dev/xvdc -R
                      - mdadm --detail --brief /dev/md0 > /etc/mdadm.conf
                      - parted /dev/md0 mklabel gpt
                      - parted /dev/md0 mkpart primary xfs 0% 100%
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: created2

    - name: remove any existing file before running just in case
      file:
        path: ./hosts.cb2
        state: absent

    - name: write instance ids and private ips of the instances to local hosts file
      lineinfile:
        dest: ./hosts.cb2
        line: "{{ item.id }} {{ item.private_ip }}"
        create: yes
      with_items: "{{ created2.instances }}"

    - name: create identifier sequence for tagging
      debug:
        msg: "{{ item }}"
      with_sequence: start={{ startindex2 }} end={{ endindex2 }} stride={{ stride2 }} format=%02d
      register: sequence2

    - name: tag instances
      ec2_tag:
        resource: "{{ item.0.id }}"
        region: "{{ region }}"
        tags:
          Name: "{{ prefix }}-{{ env }} {{ item.1.msg }}"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
      with_together:
        - "{{ created2.instances }}"
        - "{{ sequence2.results }}"

    - name: create dns records
      route53:
        command: create
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}{{ item.1.msg }}.{{ dd_env }}.{{ fqdn_suffix }}"
        type: CNAME
        ttl: 60
        value: "{{ item.0.private_dns_name }}"
        overwrite: true
      with_together:
        - "{{ created2.instances }}"
        - "{{ sequence2.results }}"

    - name: clean up after ourselves, remove the temp hosts file
      file:
        path: ./hosts.cb2
        state: absent

    - name: add new instances to host group
      add_host:
        hostname: "{{ item.private_ip }}"
        groupname: couchbase-nodes
      with_items:
        - "{{ created1.instances }}"
        - "{{ created2.instances }}"

    - name: wait for SSH to come up
      wait_for:
        host: "{{ item.private_ip }}"
        port: 22
        delay: 60
        timeout: 300
        state: started
      with_items:
        - "{{ ec2.instances }}"
        - "{{ created1.instances }}"
        - "{{ created2.instances }}"


- name: add raid and install couchbase
  vars_files:
    - ../group_vars/stg
    - ../group_vars/vaults/stg
    - ../host_vars/cbjpt-stg
  hosts: couchbase-main,couchbase-nodes
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: format /dev/md0 as xfs
      filesystem:
        fstype: xfs
        dev: /dev/md0
    
    - name: mount /opt/couchbase
      mount:
        name: /opt/couchbase
        src: /dev/md0
        fstype: xfs
        opts: noatime,relatime
        state: mounted
    
    - name: install the pre-downloaded couchbase
      yum:
        name: /root/couchbase-server-community-4.5.1-centos7.x86_64.rpm
        state: present
        disablerepo: '*'

- name: wait to be sure all couchbases are listening on 8091
  hosts: couchbase-main,couchbase-nodes
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: ensure healthy couchbase service
      wait_for:
        port: 8091


- name: Initialize the cluster and add the nodes to the cluster
  vars_files:
    - ../group_vars/stg
    - ../group_vars/vaults/stg
    - ../host_vars/cbjpt-stg
  hosts: couchbase-main
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
#    - name: Add user via curl as workaround while hearing crickets from couchbase support, mysteriously fixed itself without a version change from couchbase who admit to nothing.  Keep this around as things may break in the future.
#
#      shell: "curl -s -u {{ cb_admin_user }}:{{ cb_admin_pw }} -X POST http://localhost:8091/pools/default -d memoryQuota={{ memory_quota }}"

    - name: Configure main node
      shell: "/opt/couchbase/bin/couchbase-cli cluster-init -c 127.0.0.1:8091 --cluster-init-username={{ cb_admin_user }} --cluster-init-password={{ cb_admin_pw }} --cluster-init-port=8091 --cluster-init-ramsize={{ cluster_ram_quota }} --cluster-index-ramsize={{ cluster_index_ramsize }} --services=data,index,query,fts"

    - name: add nodes
      shell: "/opt/couchbase/bin/couchbase-cli server-add -c 127.0.0.1:8091 -u {{ cb_admin_user }} -p {{ cb_admin_pw }} --server-add={{ hostvars[item]['ansible_eth0']['ipv4']['address'] }}:8091 --server-add-username={{ cb_admin_user }} --server-add-password={{ cb_admin_pw }}"
      with_items: "{{ groups['couchbase-nodes'] }}"

    - name: Rebalance the cluster
      shell: /opt/couchbase/bin/couchbase-cli rebalance -c 127.0.0.1:8091 -u "{{ cb_admin_user }}" -p "{{cb_admin_pw }}"
    
    - name: create bucket 1 with replicas
      shell: "/opt/couchbase/bin/couchbase-cli bucket-create -c 127.0.0.1:8091 --bucket={{ bucket1_name }} --bucket-type=couchbase --bucket-port=11211 --bucket-ramsize={{ bucket1_ram_quota }}  --bucket-replica={{ bucket1_num_replicas }} -u {{ cb_admin_user }} -p {{ cb_admin_pw }}"

    - name: create bucket 2 with replicas
      shell: "/opt/couchbase/bin/couchbase-cli bucket-create -c 127.0.0.1:8091 --bucket={{ bucket2_name }} --bucket-type=couchbase --bucket-port=11211 --bucket-ramsize={{ bucket2_ram_quota }}  --bucket-replica={{ bucket2_num_replicas }} -u {{ cb_admin_user }} -p {{ cb_admin_pw }}"

