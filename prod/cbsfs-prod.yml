# Runs as ansible-playbook -vvvv cbpng-prod.yml --vault-password-file ~/.vault_pass.txt -e 'alt_num=3'
---
- hosts: localhost
  tasks:
    - name: bailout if no alt_num set
      fail:
        msg: "Bailing out you must specify -e 'alt_num=3'  ex: for prefix-env 301-30x"
      when: alt_num is not defined

- name: Creation of "{{ prefix }}-{{ env }} {{ alt_num }}01" Couchbase cluster
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    region: "{{ region }}"
    ami: "{{ cb_45_ami }}"
# Adjust the counts, and start/stop indexes to adjust cluster size
# node 1 is stand alone for cluster creation so startindex1=3 and count1 is 1 less than 
# count2
    count1: 5
    stride1: 2
    startindex1: 3
    endindex1: 11
    count2: 6
    stride2: 2
    startindex2: 2
    endindex2: 12
  vars_files:
    - ../group_vars/prod
    - ../group_vars/vaults/prod
    - ../host_vars/cbsfs-prod
  tasks:
    # First instance has to be it's own group for couchbase cluster config further in the playbook
    - name: Launch instance for master node
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_1 }}"
        zone: "{{ az1 }}"
        count: 1
        volumes:
          - device_name: /dev/xvdb
            volume_size: 800
            ephemeral: ephemeral0
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} {{ alt_num }}01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          couchbase-profile: index
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                    runcmd:
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: ec2

    - name: register dns
      route53:
        command: create
        overwrite: true
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}{{ alt_num }}01.{{ env }}.ddc.io"
        type: CNAME
        ttl: 60
        value: "{{ item.private_dns_name }}"
      with_items: "{{ ec2.instances }}"

    - name: Add EP2 instances to host group
      add_host:
        hostname: "{{ item.private_ip }}"
        groupname: couchbase-main
      with_items: "{{ ec2.instances }}"

      # These are the remaining odd numbered instances that will not be the couchbase-main in cluster
    - name: Launch instances group 1
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_1 }}"
        zone: "{{ az1 }}"
        count: "{{ count1 }}"
        volumes:
          - device_name: /dev/xvdb
            volume_size: 800
            ephemeral: ephemeral0
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} {{ alt_num }}01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          couchbase-profile: data
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                    runcmd:
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: created1

    - name: remove any existing file before running just in case
      file:
        path: ./hosts.cb1
        state: absent

    - name: write instance ids and private ips of the instances to local hosts file
      lineinfile:
        dest: ./hosts.cb1
        line: "{{ item.id }} {{ item.private_ip }}"
        create: yes
      with_items: "{{ created1.instances }}"

      # This is done in the old format because of a ansible bug that still exists in version 2.4
    - name: create identifier sequence for tagging
      debug:
        msg: "{{ item }}"
      with_sequence: start={{ startindex1 }} end={{ endindex1 }} stride={{ stride1 }} format=%02d
      register: sequence1

    - name: tag instances
      ec2_tag:
        resource: "{{ item.0.id }}"
        region: "{{ region }}"
        tags:
          Name: "{{ prefix }}-{{ env }} {{ alt_num }}{{ item.1.msg }}"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
      with_together:
        - "{{ created1.instances }}"
        - "{{ sequence1.results }}"

    - name: create dns records
      route53:
        command: create
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}{{ alt_num }}{{ item.1.msg }}.{{ dd_env }}.{{ fqdn_suffix }}"
        type: CNAME
        ttl: 60
        value: "{{ item.0.private_dns_name }}"
        overwrite: true
      with_together:
        - "{{ created1.instances }}"
        - "{{ sequence1.results }}"

    - name: clean up after ourselves, remove the temp hosts file
      file:
        path: ./hosts.cb1
        state: absent

      # These are the even numbered instances 
    - name: Launch instances group 2
      ec2:
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ subnet_pri_2 }}"
        zone: "{{ az2 }}"
        count: "{{ count2 }}"
        volumes:
          - device_name: /dev/xvdb
            volume_size: 800
            ephemeral: ephemeral0
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}-{{ env }} {{ alt_num }}01"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
          couchbase-profile: data
          icinga-profile: "{{ icinga_profile }}"
        user_data: |
                    #cloud-config
                    mounts:
                      - [ ephemeral0, null ]
                    runcmd:
                      - /bin/echo -e '\nappend domain-search "ec2.internal", "{{ dd_env }}.doubledowncasino.com", "{{ dd_env }}.ddc.io";' >> /etc/dhcp/dhclient-eth0.conf
                      - service network restart
                      - cd /usr/local/share/DDI/ddi-ops && git pull
                      - /root/bin/aws_hostname_by_tag.sh
                      - /root/bin/sysconfig setup_icinga_agent
        wait: yes
      register: created2

    - name: remove any existing file before running just in case
      file:
        path: ./hosts.cb2
        state: absent

    - name: write instance ids and private ips of the instances to local hosts file
      lineinfile:
        dest: ./hosts.cb2
        line: "{{ item.id }} {{ item.private_ip }}"
        create: yes
      with_items: "{{ created2.instances }}"

    - name: create identifier sequence for tagging
      debug:
        msg: "{{ item }}"
      with_sequence: start={{ startindex2 }} end={{ endindex2 }} stride={{ stride2 }} format=%02d
      register: sequence2

    - name: tag instances
      ec2_tag:
        resource: "{{ item.0.id }}"
        region: "{{ region }}"
        tags:
          Name: "{{ prefix }}-{{ env }} {{ alt_num }}{{ item.1.msg }}"
          app: "{{ app_tag }}"
          env: "{{ ic_env }}"
      with_together:
        - "{{ created2.instances }}"
        - "{{ sequence2.results }}"

    - name: create dns records
      route53:
        command: create
        zone: "{{ rt53_zone }}"
        record: "{{ fqdn_prefix }}{{ alt_num }}{{ item.1.msg }}.{{ dd_env }}.{{ fqdn_suffix }}"
        type: CNAME
        ttl: 60
        value: "{{ item.0.private_dns_name }}"
        overwrite: true
      with_together:
        - "{{ created2.instances }}"
        - "{{ sequence2.results }}"

    - name: clean up after ourselves, remove the temp hosts file
      file:
        path: ./hosts.cb2
        state: absent

    - name: add new instances to host group
      add_host:
        hostname: "{{ item.private_ip }}"
        groupname: couchbase-nodes
      with_items:
        - "{{ created1.instances }}"
        - "{{ created2.instances }}"

    - name: wait for SSH to come up
      wait_for:
        host: "{{ item.private_ip }}"
        port: 22
        delay: 60
        timeout: 300
        state: started
      with_items:
        - "{{ ec2.instances }}"
        - "{{ created1.instances }}"
        - "{{ created2.instances }}"


- name: add raid and install couchbase
  vars_files:
    - ../group_vars/prod
    - ../group_vars/vaults/prod
    - ../host_vars/cbsfs-prod
  hosts: couchbase-main,couchbase-nodes
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: format /dev/md0 as xfs
      filesystem:
        fstype: xfs
        dev: /dev/xvdb
    
    - name: mount /opt/couchbase
      mount:
        name: /opt/couchbase
        src: /dev/xvdb
        fstype: xfs
        opts: noatime,relatime
        state: mounted
    
    - name: install the pre-downloaded couchbase
      yum:
        name: /root/couchbase-server-community-4.5.1-centos7.x86_64.rpm
        state: present
        disablerepo: '*'

- name: wait to be sure all couchbases are listening on 8091
  hosts: couchbase-main,couchbase-nodes
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: ensure healthy couchbase service
      wait_for:
        port: 8091


- name: Initialize the cluster and add the nodes to the cluster
  vars_files:
    - ../group_vars/prod
    - ../group_vars/vaults/prod
    - ../host_vars/cbsfs-prod
  hosts: couchbase-main
  remote_user: centos
  become: yes
  become_user: root
  become_method: sudo
  tasks:
    - name: Configure main node
      shell: "/opt/couchbase/bin/couchbase-cli cluster-init -c 127.0.0.1:8091 --cluster-init-username={{ cb_admin_user }} --cluster-init-password={{ cb_admin_pw }} --cluster-init-port=8091 --cluster-init-ramsize={{ cluster_ram_quota }} --cluster-index-ramsize={{ cluster_index_ramsize }} --cluster-fts-ramsize={{ --cluster_fts_ramsize }} --services=data,index,query,fts"

    - name: add read only user
      shell:  "/opt/couchbase/bin/couchbase-cli user-manage -c 127.0.0.1:8091 -u {{ cb_admin_user }} -p {{ cb_admin_pw }} --ro-username={{ cb_ro_user }} --ro-password={{ cb_ro_pw }} --set"

    - name: add nodes
      shell: "/opt/couchbase/bin/couchbase-cli server-add -c 127.0.0.1:8091 -u {{ cb_admin_user }} -p {{ cb_admin_pw }} --server-add={{ hostvars[item]['ansible_eth0']['ipv4']['address'] }}:8091 --server-add-username={{ cb_admin_user }} --server-add-password={{ cb_admin_pw }}"
      with_items: "{{ groups['couchbase-nodes'] }}"

    - name: Rebalance the cluster
      shell: /opt/couchbase/bin/couchbase-cli rebalance -c 127.0.0.1:8091 -u "{{ cb_admin_user }}" -p "{{cb_admin_pw }}"
    
    - name: create bucket 1 with replicas
      shell: "/opt/couchbase/bin/couchbase-cli bucket-create -c 127.0.0.1:8091 --bucket={{ bucket1_name }} --bucket-type=couchbase --bucket-port=11211 --bucket-ramsize={{ bucket1_ram_quota }}  --bucket-replica={{ bucket1_num_replicas }} -u {{ cb_admin_user }} -p {{ cb_admin_pw }}"


