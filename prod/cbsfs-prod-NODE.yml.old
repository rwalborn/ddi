---
# Runs as ansible-playbook -vvvv -i 'localhost,' cbsfs-prod-NODE.yml --vault-password-file ~/.vault_pass.txt

###############################################################################
#  1 NODE CREATION FOR CLUSTER NODE REPLACEMENT                               #
#                                                                             #
#  YOU MUST UNCOMMENT THIS LINE IN ORDER FOR THIS TO RUN                      #
#   - ../host_vars/cbsfs-prod                                                     #
#  IN ORDER FOR THIS TO RUN                                                   #
#  THIS WAS DONE TO PREVENT ACCIDENTALLY RUNNING THIS AGAINST A LIVE CLUSTER  #
#                                                                             #
#  RECOMMENT THE LINE AFTER RUNNING SO IT DOES NOT ACCIDENTALLY GET CHECKED   #
#  BACK INTO SOURCE CONTROL                                                   #
#                        *** YOU HAVE BEEN WARNED ***                         #
#                                                                             #
###############################################################################

- name: Creation of "{{ prefix }}-{{ env }}" Couchbase node
  hosts: localhost
  connection: local
  gather_facts: false
  vars_files:
    - ../group_vars/prod
    - ../group_vars/vaults/prod
    - ../host_vars/cbsfs-prod
  vars:
    region: "{{ region }}"
    ami: "{{ cb_4x_ami }}"
    number: 701  # CHANGE THIS TO AN UNUSED CLUSTER NODE NUMBER
    az: "{{ az1 }}"  # CHANGE THIS TO MATCH THE AZ OF NODE BEING REPLACED
    sub_id: "{{ subnet_pri_1c }}" # CHANGE THIS BASED ON AZ - 1c = az1 and 1d = az2
  tasks:
    - name: Launch instances
      local_action:
        module: ec2
        region: "{{ region }}"
        key_name: "{{ key_name }}"
        group: "{{ sg }}"
        instance_type: "{{ instance_type }}"
        instance_profile_name: "{{ profile_name }}"
        image: "{{ ami }}"
        vpc_subnet_id: "{{ sub_id }}"
        zone: "{{ az }}"
        count: 1
        volumes:
          - device_name: /dev/xvdb
            volume_size: 40
            ephemeral: ephemeral0
            delete_on_termination: true
          - device_name: /dev/xvdc
            volume_size: 40
            ephemeral: ephemeral1
            delete_on_termination: true
        instance_tags:
          Name: "{{ prefix }}{{ number }}-{{ env }}"
          env: "{{ dd_env }}"
        wait: yes
      register: ec2

    - name: Add EP2 instances to host group
      local_action: add_host hostname={{ item.private_ip }} groupname=couchbase-node
      with_items: "{{ec2.instances}}"

    - name: register dns
      route53:
        command: create
        overwrite: true
        zone: ddc.io
        record: "{{ fqdn_prefix }}{{ number }}.{{ dd_env }}.ddc.io"
        type: A
        ttl: 60
        value: "{{ item.private_ip }}"
      with_items: "{{ ec2.instances }}"


    - name: Wait for SSH to be available
      wait_for: host={{ item.private_ip }} port=22 delay=60 timeout=300 state=started
      with_items: "{{ec2.instances}}"


- name: add raid and install couchbase and nrpe
  vars_files:
    - ../group_vars/prod
    - ../group_vars/vaults/prod
    - ../host_vars/cbsfs-prod
  hosts: couchbase-node
  become: yes
  become_user: root
  become_method: sudo
  roles:
    - cb_raid0
    - nrpe

- name: wait to be sure all couchbases are listening on 8091
  hosts: couchbase-node
  tasks:
    - name: ensure healthy couchbase service
      wait_for:
        port: 8091

